{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lars_util.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KYCwFAanAMzt"
      },
      "outputs": [],
      "source": [
        "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Enable Layer-wise Adaptive Rate Scaling optimizer in ResNet.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "def poly_rate_schedule(current_epoch,\n",
        "                       params):\n",
        "  \"\"\"Handles linear scaling rule, gradual warmup, and LR decay.\n",
        "  The learning rate starts at 0, then it increases linearly per step.  After\n",
        "  FLAGS.poly_warmup_epochs, we reach the base learning rate (scaled to account\n",
        "  for batch size). The learning rate is then decayed using a polynomial rate\n",
        "  decay schedule with power 2.0.\n",
        "  Args:\n",
        "    current_epoch: `Tensor` for current epoch.\n",
        "    params: Parameter set for Resnet model.\n",
        "  Returns:\n",
        "    A scaled `Tensor` for current learning rate.\n",
        "  \"\"\"\n",
        "\n",
        "  batch_size = params['train_batch_size']\n",
        "\n",
        "  if batch_size < 2000:\n",
        "    tf.logging.warn('LARS is not recommended for batch sizes smaller than 2000')\n",
        "  if batch_size < 16384:\n",
        "    plr = 10.0\n",
        "    w_epochs = 5\n",
        "  elif batch_size < 32768:\n",
        "    plr = 25.0\n",
        "    w_epochs = 5\n",
        "  elif batch_size < 65536:\n",
        "    plr = 32.0\n",
        "    w_epochs = 14\n",
        "  else:\n",
        "    plr = 35.6\n",
        "    w_epochs = 40\n",
        "\n",
        "  # Override default poly learning rate and warmup epochs\n",
        "  if params['poly_rate'] > 0.0:\n",
        "    plr = params['poly_rate']\n",
        "\n",
        "  wrate = (plr * current_epoch / w_epochs)\n",
        "  w_steps = (w_epochs *\n",
        "             params['num_train_images'] //\n",
        "             params['train_batch_size'])\n",
        "  min_step = tf.constant(1, dtype=tf.int64)\n",
        "  global_step = tf.train.get_or_create_global_step()\n",
        "  decay_steps = tf.maximum(min_step, tf.subtract(global_step, w_steps))\n",
        "  poly_rate = tf.train.polynomial_decay(\n",
        "      plr,\n",
        "      decay_steps,\n",
        "      params['train_steps'] - w_steps + 1,\n",
        "      power=2.0)\n",
        "  decay_rate = tf.where(current_epoch <= w_epochs, wrate, poly_rate)\n",
        "  return decay_rate\n",
        "\n",
        "\n",
        "def init_lars_optimizer(current_epoch, params):\n",
        "  \"\"\"Initialize the LARS Optimizer.\"\"\"\n",
        "\n",
        "  try:\n",
        "    from tf.contrib.opt import LARSOptimizer  # pylint: disable=g-import-not-at-top\n",
        "  except ImportError as e:\n",
        "    logging.exception('LARS optimizer is not supported in TensorFlow 2.x')\n",
        "    raise e\n",
        "\n",
        "  learning_rate = poly_rate_schedule(current_epoch, params)\n",
        "  optimizer = LARSOptimizer(\n",
        "      learning_rate,\n",
        "      momentum=params['momentum'],\n",
        "      weight_decay=params['weight_decay'],\n",
        "      skip_list=['batch_normalization', 'evonorm', 'bias'])\n",
        "  return optimizer"
      ]
    }
  ]
}